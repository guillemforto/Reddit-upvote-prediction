{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit upvote prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Dataframe manipulation\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 50) # to see more lines\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import pickle\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from itertools import groupby\n",
    "\n",
    "# Text mining\n",
    "    # cleaning\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "    # tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from emoji import UNICODE_EMOJI\n",
    "from nltk import tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Graphs\n",
    "import networkx as nx\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# URLs analysis\n",
    "import re\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "import math\n",
    "\n",
    "# for exportation\n",
    "import csv\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/guillemforto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"comments_students.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148848"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df.link_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4234970, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>created_utc</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>1430438401</td>\n",
       "      <td>1430438401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ups</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>subreddit_id</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>link_id</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>t3_34ffo5</td>\n",
       "      <td>t3_34aqsn</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>name</td>\n",
       "      <td>t1_cqug90j</td>\n",
       "      <td>t1_cqug90k</td>\n",
       "      <td>t1_cqug90z</td>\n",
       "      <td>t1_cqug91c</td>\n",
       "      <td>t1_cqug91e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>subreddit</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id</td>\n",
       "      <td>cqug90j</td>\n",
       "      <td>cqug90k</td>\n",
       "      <td>cqug90z</td>\n",
       "      <td>cqug91c</td>\n",
       "      <td>cqug91e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>author</td>\n",
       "      <td>jesse9o3</td>\n",
       "      <td>beltfedshooter</td>\n",
       "      <td>InterimFatGuy</td>\n",
       "      <td>JuanTutrego</td>\n",
       "      <td>dcblackbelt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>body</td>\n",
       "      <td>No one has a European accent either  because it doesn't exist. There are accents from Europe but...</td>\n",
       "      <td>That the kid ..reminds me of Kevin.   so sad :-(</td>\n",
       "      <td>NSFL</td>\n",
       "      <td>I'm a guy and I had no idea this was a thing guys did.</td>\n",
       "      <td>Mid twenties male rocking skinny jeans/pants, have a styled hair cut, and generally make a stron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>parent_id</td>\n",
       "      <td>t1_cqug2sr</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>t1_cqu80zb</td>\n",
       "      <td>t1_cqtdj4m</td>\n",
       "      <td>t1_cquc4rc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                0  \\\n",
       "created_utc                                                                                            1430438400   \n",
       "ups                                                                                                             3   \n",
       "subreddit_id                                                                                             t5_2qh1i   \n",
       "link_id                                                                                                 t3_34f9rh   \n",
       "name                                                                                                   t1_cqug90j   \n",
       "subreddit                                                                                               AskReddit   \n",
       "id                                                                                                        cqug90j   \n",
       "author                                                                                                   jesse9o3   \n",
       "body          No one has a European accent either  because it doesn't exist. There are accents from Europe but...   \n",
       "parent_id                                                                                              t1_cqug2sr   \n",
       "\n",
       "                                                             1              2  \\\n",
       "created_utc                                         1430438400     1430438400   \n",
       "ups                                                          3              5   \n",
       "subreddit_id                                          t5_2qh1i       t5_2qh1i   \n",
       "link_id                                              t3_34fvry      t3_34ffo5   \n",
       "name                                                t1_cqug90k     t1_cqug90z   \n",
       "subreddit                                            AskReddit      AskReddit   \n",
       "id                                                     cqug90k        cqug90z   \n",
       "author                                          beltfedshooter  InterimFatGuy   \n",
       "body          That the kid ..reminds me of Kevin.   so sad :-(           NSFL   \n",
       "parent_id                                            t3_34fvry     t1_cqu80zb   \n",
       "\n",
       "                                                                   3  \\\n",
       "created_utc                                               1430438401   \n",
       "ups                                                                1   \n",
       "subreddit_id                                                t5_2qh1i   \n",
       "link_id                                                    t3_34aqsn   \n",
       "name                                                      t1_cqug91c   \n",
       "subreddit                                                  AskReddit   \n",
       "id                                                           cqug91c   \n",
       "author                                                   JuanTutrego   \n",
       "body          I'm a guy and I had no idea this was a thing guys did.   \n",
       "parent_id                                                 t1_cqtdj4m   \n",
       "\n",
       "                                                                                                                4  \n",
       "created_utc                                                                                            1430438401  \n",
       "ups                                                                                                           101  \n",
       "subreddit_id                                                                                             t5_2qh1i  \n",
       "link_id                                                                                                 t3_34f9rh  \n",
       "name                                                                                                   t1_cqug91e  \n",
       "subreddit                                                                                               AskReddit  \n",
       "id                                                                                                        cqug91e  \n",
       "author                                                                                                dcblackbelt  \n",
       "body          Mid twenties male rocking skinny jeans/pants, have a styled hair cut, and generally make a stron...  \n",
       "parent_id                                                                                              t1_cquc4rc  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **created_utc**: The time of creation in UTC epoch-second format. Note that neither of these ever have a non-zero fraction.\n",
    "- **ups**: The number of upvotes. (includes own) It is the attribute to be predicted.\n",
    "- **subreddit_id**: The id of the subreddit in which the thing is located\n",
    "- **link_id**: ID of the link this comment is in\n",
    "- **name**: Fullname of comment, e.g.\\t1_c3v7f8u\"\n",
    "- **subreddit**: Subreddit of thing excluding the /r/ pre\f",
    "x. \\pics\"\n",
    "- **id**: This item's identi\f",
    "fier, e.g. \\8xwlg\"\n",
    "- **author**: The account name of the poster\n",
    "- **body**: The raw text, this is the unformatted text which includes the raw markup characters such as ** for bold. (<, >, and & are escaped)\n",
    "- **parent_id**: ID of the thing this comment is a reply to, either the link or comment in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddits: {'AskReddit'}\n",
      "Subreddit ids: {'t5_2qh1i'}\n",
      "All columns belong to Askreddit subreddit, so we delete them.\n"
     ]
    }
   ],
   "source": [
    "print(\"Subreddits:\", set(df.subreddit))\n",
    "print(\"Subreddit ids:\", set(df.subreddit_id))\n",
    "print(\"All columns belong to Askreddit subreddit, so we delete them.\")\n",
    "df.drop(['subreddit_id', 'subreddit'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of missing values per variable:\n",
      "created_utc     0.0\n",
      "ups            24.0\n",
      "link_id         0.0\n",
      "name            0.0\n",
      "id              0.0\n",
      "author          0.0\n",
      "body            0.0\n",
      "parent_id       0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('% of missing values per variable:')\n",
    "print(round(df.isna().sum() / len(df) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of null values per variable:\n",
      "created_utc     0.0\n",
      "ups            24.0\n",
      "link_id         0.0\n",
      "name            0.0\n",
      "id              0.0\n",
      "author          0.0\n",
      "body            0.0\n",
      "parent_id       0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('% of null values per variable:')\n",
    "print(round(df.isnull().sum() / len(df) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's normal to have null/missing values for ups, as this is the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length: 3218512 (76.0%)\n",
      "test length: 1016458 (24.0%)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets (because we only want to remove rows from the train set)\n",
    "mask = df.ups.isnull()\n",
    "train = df[~mask]\n",
    "test = df[mask]\n",
    "print(f'train length: {len(train)} ({round(len(train)/len(df) * 100, 2)}%)')\n",
    "print(f'test length: {len(test)} ({round(len(test)/len(df) * 100, 2)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### body\n",
    "\n",
    "Let's remove the comments that were deleted/removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 rows were removed (0.0 %)\n"
     ]
    }
   ],
   "source": [
    "init = len(train)\n",
    "train = train[train.body != '']\n",
    "train = train[train.body != '[removed]']\n",
    "# train = train[train.body != '[deleted]']\n",
    "print(f'{init - len(train)} rows were removed ({round((init - len(train))/init * 100,2)} %)')\n",
    "\n",
    "train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set contains 0 rows where body is empty (0.0 %)\n",
      "The test set contains 0 rows where body was removed (0.0 %)\n",
      "The test set contains 59207 rows where body was deleted (5.82 %)\n",
      "We can't remove them for Kaggle prediction.\n"
     ]
    }
   ],
   "source": [
    "empty_in_test = len(test[test.body == ''])\n",
    "print(f\"The test set contains {empty_in_test} rows where body is empty ({round(empty_in_test/len(test) * 100,2)} %)\")\n",
    "removed_in_test = len(test[test.body == '[removed]'])\n",
    "print(f\"The test set contains {removed_in_test} rows where body was removed ({round(removed_in_test/len(test) * 100,2)} %)\")\n",
    "deleted_in_test = len(test[test.body == '[deleted]'])\n",
    "print(f\"The test set contains {deleted_in_test} rows where body was deleted ({round(deleted_in_test/len(test) * 100,2)} %)\")\n",
    "print(\"We can't remove them for Kaggle prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### author\n",
    "\n",
    "Let's remove the unknown authors and AutoModerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows were removed (0.0 %)\n"
     ]
    }
   ],
   "source": [
    "init = len(train)\n",
    "# train = train[train.author != '[deleted]']\n",
    "# train = train[train.author != 'AutoModerator']\n",
    "print(f'{init - len(train)} rows were removed ({round((init - len(train))/init * 100,2)} %)')\n",
    "\n",
    "train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set contains 8190 rows where author is AutoModerator (0.81 %)\n",
      "The test set contains 62620 rows where author was deleted (6.16 %)\n",
      "We can't remove them for Kaggle prediction.\n"
     ]
    }
   ],
   "source": [
    "autom_in_test = len(test[test.author == 'AutoModerator'])\n",
    "print(f\"The test set contains {autom_in_test} rows where author is AutoModerator ({round(autom_in_test/len(test) * 100,2)} %)\")\n",
    "deleted_in_test = len(test[test.author == '[deleted]'])\n",
    "print(f\"The test set contains {deleted_in_test} rows where author was deleted ({round(deleted_in_test/len(test) * 100,2)} %)\")\n",
    "print(\"We can't remove them for Kaggle prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconcatenating\n",
    "df = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4234958"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average a thread (a discussion) has 28.45 comments.\n",
      "The average number of comments per author is 7.42\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print(\"On average a thread (a discussion) has\", round(len(df) / len(set(df.link_id)), 2), \"comments.\")\n",
    "print(\"The average number of comments per author is\", round(len(df) / len(set(df.author)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 148847 unique link_ids.\n",
      "There are 0 comments that have no parent_id and so are not a reply\n"
     ]
    }
   ],
   "source": [
    "# Structure\n",
    "print(\"There are\", len(set(df.link_id)), \"unique link_ids.\")\n",
    "print(\"There are\", len(df[df.parent_id == None]), \"comments that have no parent_id and so are not a reply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_utc      int64\n",
      "ups            float64\n",
      "link_id         object\n",
      "name            object\n",
      "id              object\n",
      "author          object\n",
      "body            object\n",
      "parent_id       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Types\n",
    "df['body'] = df['body'].astype(str)\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_processing(df, func, splitting_field = 'random', func_outputs_df = True):\n",
    "    \"\"\"Applies operations in parallel\"\"\"\n",
    "    num_cores = 3\n",
    "    \n",
    "    if splitting_field == 'random':\n",
    "        print('Splitting the dataframe into 100 equal parts. This should be instantaneous.')\n",
    "        num_partitions = 100\n",
    "        df_split = np.array_split(df, num_partitions)\n",
    "    else:\n",
    "        print(f'Splitting the dataframe by {splitting_field}. This can take several minutes...')\n",
    "        df_split = [df_i.reset_index(drop=True) for _, df_i in df.groupby(df[splitting_field])]\n",
    "        \n",
    "    num_tasks = len(df_split)\n",
    "    \n",
    "    print(\"Parallel processing starts!\")\n",
    "    pool = Pool(num_cores)\n",
    "    results = []\n",
    "    for i, _ in enumerate(pool.imap_unordered(func, df_split), 1):\n",
    "        sys.stderr.write('\\rdone {0:%}'.format(i / num_tasks))\n",
    "        results.append(_)\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    if func_outputs_df == False :\n",
    "        print('Appending everything in a single list')\n",
    "        final_results = [j for i in results if len(i) > 0 for j in i]\n",
    "        \n",
    "        print(\"Done!\")\n",
    "        return(final_results)\n",
    "    else:\n",
    "        print(\"Concatenating everything in our single dataframe...\")\n",
    "        df = pd.concat(results)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Features\n",
    "\n",
    "- **nb_linked_sr**: number of mentionned subreddits in a comment\n",
    "- **subjectivity**: number of 'I 's in a comment\n",
    "- **exclamation**: number of exclamation marks \"!\" in a comment\n",
    "- **nb_urls**: number of urls mentionned in a comment\n",
    "- **nb_pop_urls**: number of popular urls mentionned in a comment (see definition of popular below)\n",
    "- **word_count**: number of words in a comment\n",
    "- **vocabulary_richness**: measure for lexical richness\n",
    "- **is_quoting**: boolean to indicate if the comment is quoting another comment\n",
    "- **nb_emojis**: number of emojis in the comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nb_linked_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_linked_sr(df):\n",
    "    # When a subreddit is mentionned, it appears under the form '/r/' followed by its title\n",
    "    df['nb_linked_sr'] = df.body.apply(lambda x: len(re.findall(r\"/r/([^\\s/]+)\", str(x)))) # extracts\n",
    "    df['body'] = df.body.apply(lambda x: re.sub(r\"(/r/[^\\s/]+)\", '', str(x))) # removes subreddits\n",
    "    print(f'We found {sum(df.nb_linked_sr)} subreddits in total.')\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 202094 subreddits in total.\n"
     ]
    }
   ],
   "source": [
    "df = get_nb_linked_sr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(df):\n",
    "    df['subjectivity'] = df['body'].str.count('I ')\n",
    "    print(f\"We found {sum(df.subjectivity)} I's in comments.\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 2884138 I's in comments.\n"
     ]
    }
   ],
   "source": [
    "df = get_subjectivity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exclamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exclamation(df):\n",
    "    df['exclamation'] = df['body'].str.count('!')\n",
    "    print(f\"We found {sum(df.exclamation)} exclamation marks in all comments.\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 533566 exclamation marks in all comments.\n"
     ]
    }
   ],
   "source": [
    "df = get_exclamation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nb_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving sites from urls (urls identification)\n",
    "\n",
    "url_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "def extract_site(x):\n",
    "    \"\"\"\n",
    "    Extracts a list of sites, each retrieved from its url, from a given comment x\"\"\"\n",
    "    \n",
    "    # Removing spaces around signs\n",
    "    x = x.replace(' = ', '=')\n",
    "    x = x.replace(' : ', ':')\n",
    "    x = x.replace(' / ', '?')\n",
    "    urls = re.findall(url_regex, x)\n",
    "    sites = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            tmp = urlparse(url).netloc.split('.')\n",
    "            if 'youtu' in tmp:\n",
    "                sites.append('youtube')\n",
    "            elif 'com' in tmp:\n",
    "                sites.append(tmp[tmp.index('com') - 1])\n",
    "            elif 'www' in tmp:\n",
    "                sites.append(tmp[tmp.index('www') + 1])\n",
    "            else:\n",
    "                sites.append(tmp[-2])\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    return(sites)\n",
    "\n",
    "\n",
    "def get_linked_urls(df):\n",
    "    \"\"\" \n",
    "    Returns df with an extra column (linked_urls) with a list mentionned of sites\"\"\"\n",
    "    \n",
    "    df['linked_urls'] = df.body.apply(lambda x: extract_site(str(x)))\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def get_nb_urls(df):\n",
    "    df['nb_urls'] = df['linked_urls'].apply(lambda x: len(x))\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_linked_urls(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = get_nb_urls(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nb_pop_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_popular_sites(df):\n",
    "    \"\"\"Gets the list of popular urls.\n",
    "    Popular means that the url is among the 25% most frequently mentionned urls\"\"\"\n",
    "    \n",
    "    # From list of lists to single list containing everything\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    list_urls = df.linked_urls\n",
    "    list_urls = flatten(list_urls)\n",
    "    list_urls = pd.DataFrame(list_urls)\n",
    "    list_urls.columns = [\"urls\"]\n",
    "\n",
    "    list_urls = list_urls.urls.value_counts().index.tolist()\n",
    "\n",
    "    # position of 1st quartile (25% of the urls)\n",
    "    q1 = math.ceil(len(list_urls)/4)\n",
    "\n",
    "    # get the list of the 25% most popular urls\n",
    "    top_list_urls = list_urls[:q1]\n",
    "    print(\"We retrieved a list of the\", len(top_list_urls), 'most popular sites.')\n",
    "    print('The top 10 is:', top_list_urls[:10])\n",
    "    \n",
    "    return(top_list_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We retrieved a list of the 4190 most popular sites.\n",
      "The top 10 is: ['reddit', 'youtube', 'wikipedia', 'imgur', 'google', 'imdb', 'wordpress', 'tumblr', 'xkcd', 'wikimedia']\n"
     ]
    }
   ],
   "source": [
    "top_list_urls = find_most_popular_sites(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_pop_urls(df):\n",
    "    \"\"\"Among the urls cited in a comment, how many are popular\"\"\"\n",
    "    unique_top = set(top_list_urls)\n",
    "    nb_pop_urls = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        nb_pop_urls.append(len(set(df.loc[i, 'linked_urls']) & unique_top))\n",
    "        #nb_pop_urls.append(len([el for el in df.loc[i, 'linked_urls'] if el in top_list_urls]))\n",
    "    \n",
    "    df[\"nb_pop_urls\"] = nb_pop_urls\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    del df['linked_urls']\n",
    "      \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4234958/4234958 [01:22<00:00, 51475.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df = get_nb_pop_urls(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(x):\n",
    "    x = x.replace(' = ', '=')\n",
    "    x = x.replace(' : ', ':')\n",
    "    x = x.replace(' / ', '?')\n",
    "    return(re.sub(url_regex, '', x))\n",
    "\n",
    "df['body'] = df.body.apply(lambda x: remove_urls(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(df):\n",
    "    df['word_count'] = df.body.apply(lambda x: len(str(x).split()))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_word_count(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary_richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vocabulary_richness(comment_body):\n",
    "    \"\"\"Returns a measure of lexical richness of a list of words based on Yule's I measure (the inverse of Yule's K)\"\"\"\n",
    "    word_list = comment_body.split()\n",
    "    word_frequencies = {}\n",
    "    for w in word_list:\n",
    "        try:\n",
    "            word_frequencies[w] += 1\n",
    "        except KeyError:\n",
    "            word_frequencies[w] = 1\n",
    "    \n",
    "    measure1 = float(len(word_frequencies))\n",
    "    measure2 = sum([len(list(g))*(freq**2) for freq,g in groupby(sorted(word_frequencies.values()))])\n",
    "    \n",
    "    # measure1 is the unique number of words\n",
    "    # measure2 is the sum of the products of each observed frequency to the power of two (freq**2)\n",
    "    # and the number of words observed with that frequency len(list(g))\n",
    "    \n",
    "    try:\n",
    "        return(round((measure1**2) / (measure2-measure1), 2))\n",
    "    except ZeroDivisionError:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_richness(df):\n",
    "    #vocabulary_richness = []\n",
    "    #vocabulary_richness = [compute_vocabulary_richness(i) for i in list_tokens]\n",
    "    #df['vocabulary_richness'] = vocabulary_richness\n",
    "    df['vocabulary_richness'] = df.body.apply(lambda x: compute_vocabulary_richness(x))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_vocabulary_richness(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is_quoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_linked_sr(df):\n",
    "    # When a comment is mentionning another comment, the reference is under the form '&gt;'\n",
    "    df['is_quoting'] = df.body.str.contains('&gt;').astype(int) # extracts\n",
    "    df['body'] = df.body.apply(lambda x: str(x).replace('&gt;', '')) # removes\n",
    "    print(f'We found {sum(df.is_quoting)} quotations in total.')\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 70864 quotations in total.\n"
     ]
    }
   ],
   "source": [
    "df = get_nb_linked_sr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nb_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_emojis(df):\n",
    "    df['nb_emojis'] = df.body.apply(lambda x: len([i for i in str(x).split() if i in UNICODE_EMOJI]))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataframe into 100 equal parts. This should be instantaneous.\n",
      "Parallel processing starts!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done 100.000000%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating everything in our single dataframe...\n",
      "Done!\n",
      "We found 4390 on the whole set of comments\n"
     ]
    }
   ],
   "source": [
    "df = parallel_processing(df, get_nb_emojis)\n",
    "print(f\"We found {sum(df.nb_emojis)} on the whole set of comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text mining Based features\n",
    "\n",
    "- **token_i**: boolean to indicate if comment contains token_i (adds multiple columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize \n",
    "def tokenize_comment(df):\n",
    "    \"\"\"Returns a list of tokenized comments\"\"\"\n",
    "    tokens = [tokenize.word_tokenize(s) for s in df.body]\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "# Put to lower cases and remove punctuation\n",
    "def lower_no_punctuation(list_tokens):\n",
    "    tokens_no_punct_lower = []\n",
    "    for s in list_tokens: \n",
    "        tokens_no_punct_lower.append([w.lower() for w in s if w.isalpha()])\n",
    "    return(tokens_no_punct_lower)\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"ca\",\"http\"])\n",
    "def no_stop_words(list_tokens):    \n",
    "    tokens_no_stop_word = []\n",
    "    for elt in list_tokens: \n",
    "        tokens_no_stop_word.append([tok for tok in elt if tok not in stop_words])\n",
    "    return(tokens_no_stop_word)\n",
    "\n",
    "\n",
    "# Correct spelling\n",
    "def reduce_lengthening(text):\n",
    "    \"\"\"Ex: skiiin => skin\"\"\"\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "\n",
    "def correct_spelling(list_tokens):\n",
    "    spell = SpellChecker()\n",
    "    tokens_corrected = []\n",
    "    for elt in list_tokens: \n",
    "        tokens_corrected.append([reduce_lengthening(word) for word in elt])\n",
    "    return(tokens_corrected)\n",
    "\n",
    "\n",
    "# Lemmatize\n",
    "def lemmatize_tokens(list_tokens):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens_lem = []\n",
    "    for elt in list_tokens: \n",
    "        tokens_lem.append([lemmatizer.lemmatize(tok) for tok in elt])\n",
    "    return(tokens_lem)\n",
    "\n",
    "\n",
    "# Stem\n",
    "def stemming_tokens(list_tokens):\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = []\n",
    "    for elt in list_tokens : \n",
    "        stemmed.append([porter.stem(tok) for tok in elt])\n",
    "    return(stemmed)\n",
    "\n",
    "\n",
    "# Back to strings\n",
    "def reverse_tokenize_sentence(sentence):\n",
    "    sentence_as_string = ''\n",
    "    for word in sentence:\n",
    "        sentence_as_string += word + ' '\n",
    "    return(sentence_as_string.strip())\n",
    "\n",
    "\n",
    "# Main function for cleaning tokens\n",
    "def main_cleaning(list_tokens):\n",
    "    list_tokens = lower_no_punctuation(list_tokens)\n",
    "    list_tokens = no_stop_words(list_tokens)\n",
    "    list_tokens = correct_spelling(list_tokens)\n",
    "    list_tokens = lemmatize_tokens(list_tokens)\n",
    "    list_tokens = stemming_tokens(list_tokens)\n",
    "    list_tokens = [reverse_tokenize_sentence(sentence) for sentence in list_tokens]\n",
    "    return(list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8327, 8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply transformations only on comments with ups > 1000 or ups < -100\n",
    "subdf = df[(df.ups > 1000) | (df.ups < -100)]\n",
    "subdf['body'].dropna(inplace=True)\n",
    "subdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_from_subdf = tokenize_comment(subdf)\n",
    "clean_subdf_body = main_cleaning(tokens_from_subdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.5, max_features=25,\n",
       "                min_df=10, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                   max_features=25, # maximum number of words kept per comment\n",
    "                                   max_df=0.5, # don't keep words which appear in more than 0.5 of the comments\n",
    "                                   min_df=10) # don't keep words which appear in less than 10 comments\n",
    "\n",
    "tfidf_vectorizer.fit(clean_subdf_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens = tokenize_comment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4234958"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_body = pd.DataFrame(main_cleaning(list_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4234958"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_body.columns = ['body_cleansed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hundred_features = tfidf_vectorizer.transform(clean_body.body_cleansed).toarray()\n",
    "token_features = pd.DataFrame(one_hundred_features)\n",
    "token_features.columns = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features[\"id\"] = df.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features.to_pickle(\"token_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Based Features\n",
    "- **senti_neg**: negative score of comment\n",
    "- **senti_neu**: neutral score of comment\n",
    "- **senti_pos**: positive score of comment\n",
    "- **senti_comp**: compound score of comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments(df):\n",
    "    df['sentiment'] = df['body'].apply(lambda x: analyser.polarity_scores(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataframe into 100 equal parts. This should be instantaneous.\n",
      "Parallel processing starts!\n"
     ]
    }
   ],
   "source": [
    "# takes approx 15 mins\n",
    "sentiment = parallel_processing(df, get_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['senti_neg'] = sentiment['sentiment'].apply(lambda x: x['neg'])\n",
    "df['senti_neu'] = sentiment['sentiment'].apply(lambda x: x['neu'])\n",
    "df['senti_pos'] = sentiment['sentiment'].apply(lambda x: x['pos'])\n",
    "df['senti_comp'] = sentiment['sentiment'].apply(lambda x: x['compound'])\n",
    "del sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural features\n",
    "- **nb_author_thread**: number of unique authors in the link_id of the comment (gives a sense of thread popularity)\n",
    "- **is_root**: whether or not the parent comment is a direct reply to the first comment of an Askreddit subreddit\n",
    "- **depth**: depth of a comment in a thread\n",
    "- **nb_neighbours**: degree (number of neighbours) of a comment in a thread\n",
    "- **author_centralities (3 measures)**: centrality metrics of the author (intuition: the more connected an author, the more important it is)\n",
    "- **nb_com_author**: number of comments per author (gives a sense of author activity)\n",
    "- **order_com**: the rank of the comment based on time\n",
    "- **parent_score**: the score of the parent of the comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nb_author_thread\n",
    "\n",
    "For the features to come, we will often need to get a dictionnary from two columns of our pandas DataFrame (one for the keys and the other for some aggregation of the values). Once we have it, we will try to get values based on keys from that dictionnary. Hence, this function will be handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_to_get_from_dico(dico, x):\n",
    "    try:\n",
    "        return dico[x]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_author_thread(df):\n",
    "    link_id_to_nbauthors = pd.Series(df.author.values, index=df.link_id).groupby(by=['link_id']).count().to_dict()\n",
    "    df['nb_author_thread'] = df.link_id.apply(lambda x: try_to_get_from_dico(link_id_to_nbauthors, x))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_nb_author_thread(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 1702201 comments in the root in total (40.19%).\n"
     ]
    }
   ],
   "source": [
    "df['is_root'] = df.parent_id.str.startswith('t3_').astype(int)\n",
    "print(f'We found {sum(df.is_root)} comments in the root in total ({round(sum(df.is_root) / len(df) * 100, 2)}%).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### depth and nb_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_and_nb_neighbours(df):\n",
    "    \"\"\"In order to get the depth and the number of neighbours of each comment of the dataframe, \n",
    "creates a network from it, where a node is a comment, and two nodes are connected if they have \n",
    "a parent-child relationship.\n",
    "\n",
    "    Parameters:\n",
    "    df (dataframe)   : dataframe\n",
    "\n",
    "    Returns:\n",
    "    df               : dataframe (with depth and nb_neighbours)\"\"\"\n",
    "    \n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(df.link_id, type = \"link\")\n",
    "    g.add_nodes_from(df.name, type = \"comment\")\n",
    "    g.add_edges_from(df[[\"parent_id\", \"name\"]].values, linktype = \"parent\")\n",
    "    \n",
    "    df['nb_neighbours'] = [g.degree[n] for n in df['name']]\n",
    "    df['depth'] = [len(nx.ancestors(g, n)) for n in df['name']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillemforto/anaconda3/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if not cb.iterable(width):\n",
      "/Users/guillemforto/anaconda3/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if cb.iterable(node_size):  # many node sizes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting 200 nodes of the 1st layer (the comments that were replying to the original comment) of the 1st link_id\n",
    "\n",
    "# subset\n",
    "small_df = df[df.link_id == df.link_id[0]]\n",
    "small_df.reset_index(drop=True, inplace=True)\n",
    "little_graph = nx.DiGraph()\n",
    "little_graph.add_nodes_from(small_df.link_id, type = \"link\")\n",
    "little_graph.add_nodes_from(small_df.name, type = \"comment\")\n",
    "little_graph.add_edges_from(small_df[[\"parent_id\", \"name\"]].values, linktype = \"parent\")\n",
    "    \n",
    "# prepare options\n",
    "colorNode = [0]\n",
    "nodes_to_plot = [df.link_id[0]] # labels\n",
    "for source, target, attributes in little_graph.edges.data():\n",
    "    nb_ancestors = len(nx.ancestors(little_graph, target))\n",
    "    if nb_ancestors <= 1: \n",
    "        nodes_to_plot.append(target)\n",
    "        colorNode.append(nb_ancestors)\n",
    "\n",
    "nodes_to_plot = nodes_to_plot[:200]\n",
    "colorNode = colorNode[:200]\n",
    "super_little_graph = little_graph.subgraph(nodes_to_plot)\n",
    "\n",
    "# plot\n",
    "options = {\n",
    "    'node_color' : colorNode,\n",
    "    'node_size' : 1000, \n",
    "    'cmap' : plt.get_cmap(\"jet\"),\n",
    "    'node_shape' : 'o',\n",
    "    'with_labels' : True, \n",
    "    \"width\" : 0.1, \n",
    "    \"font_size\" : 15,\n",
    "    \"nodelist\" : nodes_to_plot,\n",
    "    \"alpha\" : 0.8   \n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 14))\n",
    "nx.draw(super_little_graph, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataframe by link_id. This can take several minutes...\n",
      "Parallel processing starts!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done 100.000000%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating everything in our single dataframe...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df = parallel_processing(df, get_depth_and_nb_neighbours, splitting_field = 'link_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### author_centralities\n",
    "\n",
    "From Kaggle: \"another way to model your data could be to define a user-user undirected network (instead of a comment-comment like we just did). In this network, a node is a user and there exists a link between two users if one has answered a comment from the other.\n",
    "The idea is to extract communities of users and perhaps calculte some centrality metrics (to get the authority of the author for instance).\"\n",
    "\n",
    "Of course, we also want to take into account the amount of interactions between two users, wo we will add this through a weight attribute in our graph.\n",
    "\n",
    "We will hence compute the following centrality metrics:\n",
    "- **degree centrality**: for a node v it's the fraction / the number of nodes it is connected to ;\n",
    "- **betweenness_centrality**: for a node v it's the fraction / the number of all-pairs shortest paths that pass through v ;\n",
    "- **eigenvector_centrality**: it computes the centrality of a node based on the centrality of its neighbors (recursive algo) using the adjancecy matrix.\n",
    "\n",
    "Three remarks:\n",
    "- Apart from the three metrics, we also tried to compute closeness_centrality, which uses the very greedy (complexity of O(n log(n) + m) with n the number of nodes and m the number of edges) Dijkstra's algorithm to find shortest paths. As our graph isn't connected (see below), NetworkX's algorithm computes it for every connected part, but due to too many nodes, it's too slow. No big deal since we were still able to compute the three others!\n",
    "- The last two metrics (betweenness and eigenvector centrality) use the weight attribute.\n",
    "- By default, all metrics are normalized by the number of nodes in the connected part of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_network(df):\n",
    "    sub_df1 = df[['author', 'name']]\n",
    "    sub_df2 = df[['author', 'parent_id']]\n",
    "\n",
    "    # We want the intersections with no NA so we choose inner merge\n",
    "    print('Finding nodes to be connected. This will take a minute...')\n",
    "    df1_df2_merge = sub_df1.merge(sub_df2, left_on='name', right_on='parent_id', how='inner',\n",
    "                                  suffixes = ('_parent', '_child'))\n",
    "    \n",
    "    # We have parents on the left and childs on the right\n",
    "    df1_df2_merge = df1_df2_merge.groupby(['author_parent', 'author_child']).aggregate({'author_child':'count'})\n",
    "    df1_df2_merge.columns = ['counts']\n",
    "    df1_df2_merge.reset_index(level=0, drop=False, inplace=True)\n",
    "    df1_df2_merge.reset_index(level=0, drop=False, inplace=True)\n",
    "    \n",
    "    nodes_to_be_connected = df1_df2_merge.values\n",
    "    \n",
    "    # Graph (Undirected)\n",
    "    g = nx.Graph() # Undirected Simple type\n",
    "    g.add_weighted_edges_from(nodes_to_be_connected)\n",
    "    print('Graph created!')\n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_centralities(df):\n",
    "    degree_centralities = []\n",
    "    betweenness_centralities = []\n",
    "    eigenvector_centralities = []\n",
    "    \n",
    "    print(\"Getting degree centralities...\")\n",
    "    deg = nx.degree_centrality(authors_graph)\n",
    "    \n",
    "    print(\"Getting eigenvector centralities...\")    \n",
    "    eig = nx.eigenvector_centrality(authors_graph, weight = 'weight')\n",
    "    \n",
    "    print(\"Getting betweenness centralities...\")\n",
    "    print(\"We use 50 nodes to estimate betweenness centrality so you might want to go for a coffee :D\")\n",
    "    bet = nx.betweenness_centrality(authors_graph, k = 50, weight = 'weight', normalized = True)\n",
    "    \n",
    "    #print(\"Getting closeness centralities...\")\n",
    "    #clo = nx.closeness_centrality(authors_graph, distance = 'weight', wf_improved = True)\n",
    "    \n",
    "    print(\"Appending the features...\")\n",
    "    all_nodes = authors_graph.nodes\n",
    "    deg_mean = int(pd.Series([*deg.values()]).mean())\n",
    "    eig_mean = int(pd.Series([*eig.values()]).mean())\n",
    "    bet_mean = int(pd.Series([*bet.values()]).mean())\n",
    "    for i in df.author :\n",
    "        if i in all_nodes:\n",
    "            degree_centralities.append(deg[i])\n",
    "            eigenvector_centralities.append(eig[i])\n",
    "            betweenness_centralities.append(bet[i])\n",
    "        else:\n",
    "            degree_centralities.append(deg_mean)\n",
    "            eigenvector_centralities.append(eig_mean)\n",
    "            betweenness_centralities.append(bet_mean)\n",
    "    \n",
    "    df['degree_centrality'] = degree_centralities\n",
    "    df['betweenness_centrality'] = betweenness_centralities\n",
    "    df['eigenvector_centrality'] = eigenvector_centralities\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding nodes to be connected. This will take a minute...\n",
      "Graph created!\n"
     ]
    }
   ],
   "source": [
    "authors_graph = user_user_network(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph is not connected.\n"
     ]
    }
   ],
   "source": [
    "if nx.is_connected(authors_graph) == False :\n",
    "    print('The graph is not connected.')\n",
    "else: \n",
    "    print('The graph is connected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_author_centralities(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nb_com_author\n",
    "In the case of nb_com_author, the keys of the dico will be our author names, and the values will be a counting of the number of comments they made. A remark: AutoModerator couldn't be removed on the cleaning phase because it is in some rows of the test, and so in order not to dramatically increase the variance of our feature, we will replace the real value by the median ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_com_author(df):\n",
    "    author_to_nbcoms = pd.Series(df.name.values, index=df.author).groupby(by=['author']).count().to_dict()\n",
    "    author_to_nbcoms['AutoModerator'] = pd.Series([*author_to_nbcoms.values()]).median() # median replacement\n",
    "    df['nb_com_author'] = df.author.apply(lambda x: try_to_get_from_dico(author_to_nbcoms, x))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_nb_com_author(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### order_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_order_com(df):\n",
    "    \"\"\"Gets the order of apparition (i.e. rank) of each comment, inside a thread, based on time it was published.\n",
    "This rank is then divided by the length of the thread to normalize the values.\n",
    "Two comments with same publication time will have the same order.\n",
    "\n",
    "    Parameters:\n",
    "    df (dataframe)   : dataframe\n",
    "\n",
    "    Returns:\n",
    "    df               : dataframe (with order_com)\"\"\"\n",
    "    \n",
    "    timestamps = df.created_utc.tolist()\n",
    "    s = sorted(timestamps)\n",
    "    df['order_com'] = [s.index(x) for x in timestamps]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataframe by link_id. This can take several minutes...\n",
      "Parallel processing starts!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done 100.000000%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating everything in our single dataframe...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df = parallel_processing(df, get_order_com, splitting_field = 'link_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parent_score\n",
    "\n",
    "To get the parent_score, we get a dictionary with the scores of every comment. Then, for every parent_id, we try to find it among the keys od the dictionary (the names), and get the score. Of course, some comments don't have a parent, so in this case we simply decided to use median imputation (mean was too high as it is too sensible to extreme values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_score(df):\n",
    "    comment_to_score = pd.Series(df['ups'].values, index=df['name']).to_dict() # dictionary\n",
    "    df['parent_score'] = df.parent_id.apply(lambda x: try_to_get_from_dico(comment_to_score, x))\n",
    "    # fill nas\n",
    "    med = df.parent_score.median()\n",
    "    df.parent_score.fillna(med, inplace=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_parent_score(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time based features\n",
    "\n",
    "- **time_lapse**: time since publication of first comment in thread\n",
    "- **hour**: hour of the day the comment was posted\n",
    "- **weekday**: day of the week the comment was published\n",
    "- **time_since_parent**: the elapsed time between the post publication and the parent publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert created_utc to datetime\n",
    "df['time'] = pd.to_datetime(df.created_utc, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time_lapse\n",
    "First of all, we will create a dictionary with publication times for first comment(s) in each thread.\n",
    "Then, we will try to get the time difference between comment publication and first comment publication.\n",
    "We didn't have any NA (logically, as there is always a first comment), so no need for median or mean imputation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_lapse(df):\n",
    "    df_first_comment_dict = df[['link_id', 'time']].groupby(by=['link_id']).min().to_dict('index') # dictionary\n",
    "    df['time_lapse'] = df.time - df.link_id.apply(lambda x: df_first_comment_dict[x]['time'])\n",
    "    df.time_lapse = df.time_lapse.apply(lambda x: x.total_seconds())\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_time_lapse(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hour and weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df.time.apply(lambda x: x.hour)\n",
    "df['weekday'] = df.time.dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time_since_parent\n",
    "\n",
    "Our appracoh here will be similar to what we did with time_lapse. It will consist in creating a dictionary with all times for each comment (i.e. for each name) at first.\n",
    "Then, we will try to get the publication time of the parent.\n",
    "Of course, if the parent_id does not correspond to any name, then time_since_parent will be missing. In that case, we will assume that if the time is missing, it means that the comment does not have a parent. Hence we will simply replace missing values by publication time, so that afterwards, when we compute the differences between both values, we will have time_since_parent = 0 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_since_parent(df):\n",
    "    times_for_each_comment = pd.Series(df.time.values, index=df.name).to_dict()\n",
    "    df['time_since_parent'] = df.parent_id.apply(lambda x: try_to_get_from_dico(times_for_each_comment, x))\n",
    "    df.loc[df.time_since_parent.isna(), 'time_since_parent'] = df.time\n",
    "    df.time_since_parent = df.time - df.time_since_parent\n",
    "    df.time_since_parent = df.time_since_parent.apply(lambda x: x.total_seconds())\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_time_since_parent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modelisation part will be in the other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df_v13.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
