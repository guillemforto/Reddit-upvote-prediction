{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Dataframe manipulation\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 50) # to see more lines\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import pickle\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from itertools import groupby\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Text mining\n",
    "    # cleaning\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "    # tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from emoji import UNICODE_EMOJI\n",
    "from nltk import tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Graphs\n",
    "import networkx as nx\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# URLs analysis\n",
    "import re\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "import math\n",
    "\n",
    "# for exportation\n",
    "import csv\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from parfit import bestFit\n",
    "\n",
    "# Boosting algorithms\n",
    "import xgboost as xgb\n",
    "\n",
    "# NN\n",
    "from tensorflow import keras\n",
    "from keras import optimizers, regularizers, callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read\n",
    "#os.chdir(\"C:\\\\Users\\Jade\\Documents\\COURS\\M2\\S2\\WebMining\\Project\")\n",
    "with open('df_v13.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging with 25 text mining features (we had to create them appart)\n",
    "with open('token_features.pkl', 'rb') as f:\n",
    "    token_features = pickle.load(f)\n",
    "\n",
    "token_features.set_index('id', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('id', inplace=True, drop=True)\n",
    "df = df.merge(token_features, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df_all_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read\n",
    "#os.chdir(\"C:\\\\Users\\Jade\\Documents\\COURS\\M2\\S2\\WebMining\\Project\")\n",
    "with open('df_all_features.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_names = list(token_features.columns)\n",
    "token_names.remove(\"time\")\n",
    "token_names.append(\"time_y\")\n",
    "del token_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the variables into three groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['nb_linked_sr', 'subjectivity', 'nb_urls', 'nb_pop_urls', \n",
    "                      'word_count', 'exclamation', 'vocabulary_richness', 'nb_emojis',\n",
    "                      'nb_author_thread', 'nb_neighbours', 'depth',\n",
    "                      'degree_centrality', 'betweenness_centrality', 'eigenvector_centrality', \n",
    "                      'order_com', 'parent_score', 'time_lapse', 'time_since_parent', 'nb_com_author']\n",
    "\n",
    "boolean_features = ['is_root', 'is_quoting']\n",
    "boolean_features.extend(token_names)\n",
    "\n",
    "categorical_features = ['hour', 'weekday']\n",
    "\n",
    "all_features = []\n",
    "all_features.extend(categorical_features)\n",
    "all_features.extend(numerical_features)\n",
    "all_features.extend(boolean_features)\n",
    "\n",
    "print(\"We have\", len(numerical_features), \"numerical features,\", \n",
    "      len(boolean_features), \"boolean features and\",\n",
    "      len(categorical_features), \"categorical features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the variable **is_root** could be misleading. It just corresponds to comments that are a reply to the initial question in the Askreddit, they are in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_features].describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_too_large = [i for i in numerical_features if df[i].std() > 5]\n",
    "variance_too_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables such as word_count, have a much larger variance than others, we need to be careful with that! We will scale these features in the data preparation step below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "To submit the Kaggle we will need to predict on test, using what we have learned on train, so we will need the test to have the same columns as test (except for the ups)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for correlation\n",
    "\n",
    "Before including our features into our model, we need to check if a variable is \"highly\" correlated with the target variable. Indeed, such variables do not necessarily improve the model so adding them would be at the expense of efficiency (higher dimensionality).\n",
    "\n",
    "Let's check the correlation between each numerical feature, including the boolean (coded by 0 and 1) with ups. \n",
    "What we usually mean by a \"high\" correlation is when the Pearson correlation coefficient between 2 numerical variables is above 0.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selection of features\n",
    "var_select = []\n",
    "var_select.extend(numerical_features)\n",
    "var_select.extend(boolean_features)\n",
    "var_select.append('ups')\n",
    "\n",
    "# Pearson correlation coefficient\n",
    "df_corr = df[var_select]\n",
    "matrix_corr = df_corr.corr(method ='pearson') \n",
    "corr_num_ups = matrix_corr['ups'][:-1]\n",
    "corr_num_ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no coefficient above 0.7, hence we can keep them all without a risk of anormally high correlation with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns for prediction\n",
    "useless_cols = [\"created_utc\", \"link_id\", \"name\", \"author\", \"body\", \"parent_id\", \"time_x\"]\n",
    "df.drop(columns=useless_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking column names\n",
    "print(df.columns, '\\n')\n",
    "\n",
    "print(\"Our column names are fine, they don't contain brackets or weird symbols.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables\n",
    "The goal here is to transform categorical variables into numerical so that they are suitable to be used in machine learning techniques. After some research, I realized there are many ways to encode, adapted to different kinds of issues such as high cardinality, ordered or nominal, etc. <br>\n",
    "In our case we don't have any variable with a lot of modalities (more than 1000) so we don't need to worry about high cardinality issues, but we still have some categorical feautres that seem really important to determine the score, so we must still be careful if we want to do it right. <br>\n",
    "The thing is that many articles in the litterature have shown that the type of encoding that we use should depend on the type of model that we intend to implement. For instance, some boosting models (such as CatBoost) are able to operate with categorical features directly, whereas neural networks or linear models require these features to be previously transformed to a numerical version (one dummy for every modality, for instance). Besides, for a tree-based model, **some papers suggest that a single level of a one-hot-encoded categorical variable must have a very high importance score (which is measured by how much it decreases tree impurity) in order to be selected for splitting at an early stage (i.e. closer to the root), over a continuous variable**. And so, because of the fact that continuous variables would be more likely to be selected from the beggining, it could happen that the subsequent branches are impure and that even if we include categorical features at a later stage, it is already too late for obtaining an optimal performance....\n",
    "\n",
    "In our case we only have 2 categorical features (4 if we count the boolean ones), so we won't implement catboost. Hence, taking all of this into account, in what follows we will:\n",
    "- ordinally encode multinomial **hierarchical** categorical features, to keep the hierarchy of modalities: hour and weekday (we don't need to do anything because it's already ordinally encoded)\n",
    "- one-hot encode simple ones (non-hierarchical ones): leave booleans as integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling\n",
    "This is a necessary step. From sklearn-learn.org: \"If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\"\n",
    "Hence we will standard scale every feature with too large variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting what we are going to scale\n",
    "df_to_scale = df[variance_too_large]\n",
    "\n",
    "# Keeping all the rest\n",
    "features_to_append = [i for i in all_features if i not in variance_too_large]\n",
    "features_to_append.append('ups')\n",
    "to_append = df[features_to_append]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaling (remove mean and scale to unit variance)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_np_array = StandardScaler().fit_transform(df_to_scale.values)\n",
    "df_scaled = pd.DataFrame(scaled_np_array, index=df_to_scale.index, columns=df_to_scale.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.concat([df_scaled, to_append], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting into X, y and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_scaled.ups.isnull()\n",
    "X = df_scaled[~mask]\n",
    "y = X.ups\n",
    "X = X.drop('ups', axis=1)\n",
    "\n",
    "test = df_scaled[mask]\n",
    "X_test = test.drop('ups', axis=1) # Kaggle submission will be predictions on X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and validation sets\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_validation.to_frame().describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model):\n",
    "    \"\"\"Evaluates model performances and tells if we overfit or not, and by how much\"\"\"\n",
    "    \n",
    "    predicted_train = model.predict(X_train.values)\n",
    "    predicted_validation = model.predict(X_validation.values)\n",
    "    \n",
    "    train_MAE = round(mean_absolute_error(y_train, predicted_train), 4)\n",
    "    validation_MAE = round(mean_absolute_error(y_validation, predicted_validation), 4)\n",
    "    \n",
    "    print(\"Training MAE:\", train_MAE)\n",
    "    print(\"Validation MAE:\", validation_MAE)\n",
    "    if train_MAE < validation_MAE:\n",
    "        print(\"We overfit...\")\n",
    "    else:\n",
    "        print(\"We don't overfit!\")\n",
    "        \n",
    "    return((train_MAE, validation_MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(kaggle_prediction):\n",
    "    \"\"\"Returns a csv files with predictions in the good format for Kaggle submission\"\"\"\n",
    "    csvData = [['id', 'Predicted']]\n",
    "    indices = X_test.index.to_list()\n",
    "    print(\"Creating csv...\")\n",
    "    for i, j in zip(range(len(indices)), range(len(kaggle_prediction))):\n",
    "        csvData.append([indices[i], kaggle_prediction[j]])\n",
    "    \n",
    "    filename = 'submission_' + datetime.today().strftime('%m_%d_%H_%M') + '.csv'\n",
    "    with open(filename, 'w') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerows(csvData)\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(model):\n",
    "    ft_weights_xgb_reg = pd.DataFrame(model.feature_importances_, columns=['weight'], index=X_train.columns)\n",
    "    ft_weights_xgb_reg.sort_values('weight', inplace=True)\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    plt.barh(ft_weights_xgb_reg.index, ft_weights_xgb_reg.weight, align='center') \n",
    "    plt.title(f\"Feature importances in the {model.__class__} model\", fontsize=14)\n",
    "    plt.margins(y=0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_model_performances = {} # we'll fill this dico progressively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "For the past recent years, XGBoost has been the king algorithm delivering fast and high performance for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "xgb_reg = xgb.XGBRegressor(n_jobs=-1, random_state=1139, verbosity=2, max_depth=5)\n",
    "xgb_reg.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prediction results\n",
    "dico_model_performances['xgb_reg'] = {model_evaluation(xgb_reg)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We overfit too much, let's run some parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importances(xgb_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nb_neighbours is by far the most important feature! It's quite impressive how the difference is so huge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple parameter tunning using $k$-fold CV ($k$=5)\n",
    "\n",
    "General parameters:\n",
    "* booster (default: gbtree): type of model. Default gbtree performs always better than gblinear.\n",
    "* seed: used for reproducible results, useful for parameter tunning.\n",
    "* nthread: nb of parallel threads used.\n",
    "\n",
    "Parameters we will tune:\n",
    "* n_estimators: number of trees to fit\n",
    "* max_depth: maximum depth of a tree. Will be used to control over-fitting (higher depth => more specific tree).\n",
    "\n",
    "Other parameters redundant with what we already used (max_leaf_nodes, colsample_bylevel) or for which default value is okay (gamma, max_delta_step, etc.) won't be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": np.array([10, 60, 120]),\n",
    "    \"max_depth\": np.array([4,5,6])\n",
    "}\n",
    "\n",
    "# We start by fixing the rest to standard values\n",
    "xgb1 = xgb.XGBRegressor(\n",
    "    eta = 0.1,\n",
    "    njobs = -1,\n",
    "    seed=1139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_score, all_models, all_scores = bestFit(\n",
    "    model = xgb1,\n",
    "    paramGrid = ParameterGrid(params), \n",
    "    X_train = X_train,\n",
    "    y_train = y_train,\n",
    "    X_val = X_validation,\n",
    "    y_val = y_validation,\n",
    "    metric = mean_absolute_error,\n",
    "    # Choice between optimizing for greater scores or lesser scores Default True means greater and False means lesser\n",
    "    scoreLabel = \"mean_absolute_error\",\n",
    "    greater_is_better = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost last run: optimized parameters\n",
    "\n",
    "Here we will replace n_estimators and max_depth with the best parameters.\n",
    "\n",
    "To avoid overfitting, we will also decrease subsample (fraction of observations to be randomly sampled for each tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_n_estimators = 60\n",
    "opt_max_depth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model\n",
    "xgb_reg_star = xgb.XGBRegressor(n_estimators=opt_n_estimators,\n",
    "                                eta=0.1,\n",
    "                                max_depth=opt_max_depth,\n",
    "                                subsample=0.9,\n",
    "                                colsample_bytree=0.9,\n",
    "                                gamma=0,\n",
    "                                n_jobs=-1,\n",
    "                                verbosity=2,\n",
    "                                seed=1139)\n",
    "\n",
    "xgb_reg_star.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction results\n",
    "dico_model_performances['xgb_reg_star'] = {model_evaluation(xgb_reg_star)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_feature_importances(xgb_reg_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle prediction\n",
    "kaggle_prediction = np.round(xgb_reg_star.predict(X_test.values), 1)\n",
    "create_csv(kaggle_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building a simple NN\n",
    "nn = Sequential()\n",
    "nn.add(Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "nn.add(Dense(256, activation='relu'))\n",
    "nn.add(Dense(256, activation='relu'))\n",
    "nn.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn.compile(loss='mean_absolute_error',\n",
    "            optimizer=\"adam\",\n",
    "            metrics=['mean_absolute_error'])\n",
    "\n",
    "# Model summary\n",
    "print(nn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "early_stopping = callbacks.EarlyStopping(monitor='mean_absolute_error', patience=7, verbose=1)\n",
    "# When the model doesn't improve after 15 iterations, we stop even if it hasn't finished (saves us time).\n",
    "\n",
    "nn_history = nn.fit(X_train.values,\n",
    "                    y_train.values,\n",
    "                    batch_size=256,\n",
    "                    epochs=2,\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping],\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_model_performances['nn_base'] = {model_evaluation(nn)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle prediction\n",
    "a = np.round(nn.predict(X_test.values), 1)\n",
    "kaggle_prediction = np.array([a[i][0] for i in range(len(a))])\n",
    "create_csv(kaggle_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tunned manually. Below is the best model along with the results from experimental trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental results: (25 epochs)\n",
    "\n",
    "1. 4 layers (128, 256, 256, 1) / no regularization / no batchnormalization / default adam / batch_size=256 => (train_MAE=8.0477, validation_MAE=8.1296) Kaggle: 9.29011\n",
    "* 4 layers (128, 256, 256, 1) / dropout=0.2 / no batchnormalization / default adam / batch_size=256 => (train_MAE=7.9864, validation_MAE=8.054) Kaggle: 9.50644\n",
    "* 3 layers (128, 256, 1) / dropout=0.2 / no batchnormalization / custom adam / batch_size=256 => (train_MAE=8.1594, validation_RMSE=8.2105) Kaggle: 9.54065,\n",
    "* **4 layers (128, 256, 256, 1) / dropout=0.4 / batchnormalization / custom adam / batch_size=256 => (train_MAE=23.0333, validation_MAE=27.939)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model was the first one, but Neural Network is no better than XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_performances = [dico_model_performances[key][1] for key in dico_model_performances.keys()]\n",
    "all_performances = sorted(all_performances)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "\n",
    "ax.bar(dico_model_performances.keys(), all_performances, color=\"green\")\n",
    "plt.title('Models Comparison')\n",
    "plt.xlabel('Models')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel('Mean Absolute Error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
